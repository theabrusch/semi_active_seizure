Runnin script...
done loading packages
Initialising training dataset.
Trying to load segmentation from disk.
Succesfully loaded segmentation.
Initialising validation dataset.
Trying to load segmentation from disk.
Succesfully loaded segmentation.
Data loader initialization took 0:00:01.866244 .
BaselineCNN(
  (convblock): Sequential(
    (0): Conv2d(1, 20, kernel_size=(1, 11), stride=(1, 1), padding=same)
    (1): ELU(alpha=1.0)
    (2): Dropout(p=0.4, inplace=False)
    (3): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): Conv2d(20, 20, kernel_size=(17, 21), stride=(1, 1), padding=same)
    (5): ELU(alpha=1.0)
    (6): Dropout(p=0.4, inplace=False)
    (7): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (8): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
    (9): Conv2d(20, 40, kernel_size=(11, 21), stride=(1, 1), padding=same)
    (10): ELU(alpha=1.0)
    (11): Dropout(p=0.4, inplace=False)
    (12): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (13): MaxPool2d(kernel_size=(1, 2), stride=(1, 2), padding=0, dilation=1, ceil_mode=False)
    (14): Conv2d(40, 80, kernel_size=(11, 11), stride=(1, 1), padding=same)
  )
  (dropout): Dropout(p=0.4, inplace=False)
  (GlobAvgPool): AvgPool2d(kernel_size=(18, 128), stride=(18, 128), padding=0)
  (fc): Linear(in_features=80, out_features=2, bias=True)
)
Epoch 1 out of 90
Training loss: tensor(0.5318)
Validation loss: tensor(0.5635)
Epoch 2 out of 90
Training loss: tensor(0.5206)
Validation loss: tensor(0.5519)
Epoch 3 out of 90
Training loss: tensor(0.5320)
Validation loss: tensor(0.6108)
Epoch 4 out of 90
Training loss: tensor(0.5584)
Validation loss: tensor(0.5698)
Epoch 5 out of 90
Training loss: tensor(0.5287)
Validation loss: tensor(0.5433)
Epoch 6 out of 90
Training loss: tensor(0.5260)
Validation loss: tensor(0.6168)
Epoch 7 out of 90
Training loss: tensor(0.5249)
Validation loss: tensor(0.5722)
Epoch 8 out of 90
Training loss: tensor(0.5426)
Validation loss: tensor(0.5807)
Epoch 9 out of 90
Training loss: tensor(0.5776)
Validation loss: tensor(0.6130)
Epoch 10 out of 90
Training loss: tensor(0.5371)
Validation loss: tensor(0.6026)
Epoch 11 out of 90
Training loss: tensor(0.5338)
Validation loss: tensor(0.6309)
Epoch 12 out of 90
Training loss: tensor(0.5352)
Validation loss: tensor(0.7045)
Epoch 13 out of 90
Training loss: tensor(0.5751)
Validation loss: tensor(0.7118)
Epoch 14 out of 90
Training loss: tensor(0.5509)
Validation loss: tensor(0.5834)
Epoch 15 out of 90
Training loss: tensor(0.5295)
Validation loss: tensor(0.6119)
Epoch 16 out of 90
Training loss: tensor(0.5409)
Validation loss: tensor(0.5778)
Epoch 17 out of 90
Training loss: tensor(0.5189)
Validation loss: tensor(0.6694)
Epoch 18 out of 90
Training loss: tensor(0.5768)
Validation loss: tensor(0.5922)
Epoch 19 out of 90
Training loss: tensor(0.5865)
Validation loss: tensor(0.5943)
Epoch 20 out of 90
Training loss: tensor(0.6254)
Validation loss: tensor(0.7852)
Epoch 21 out of 90
Training loss: tensor(0.7526)
Validation loss: tensor(0.7596)
Epoch 22 out of 90
Training loss: tensor(0.5549)
Validation loss: tensor(0.5950)
Epoch 23 out of 90
Training loss: tensor(0.5536)
Validation loss: tensor(0.5773)
Epoch 24 out of 90
Training loss: tensor(0.5200)
Validation loss: tensor(0.6311)
Epoch 25 out of 90
Training loss: tensor(0.5244)
Validation loss: tensor(0.6155)
Epoch 26 out of 90
Training loss: tensor(0.5231)
Validation loss: tensor(0.5722)
Epoch 27 out of 90
Training loss: tensor(0.5254)
Validation loss: tensor(0.5456)
Epoch 28 out of 90
Training loss: tensor(0.7128)
Validation loss: tensor(0.7836)
Epoch 29 out of 90
Training loss: tensor(0.7847)
Validation loss: tensor(0.7119)
Epoch 30 out of 90
Training loss: tensor(0.5499)
Validation loss: tensor(0.5676)
Epoch 31 out of 90
Training loss: tensor(0.5833)
Validation loss: tensor(0.6863)
Epoch 32 out of 90
Training loss: tensor(0.6641)
Validation loss: tensor(0.7447)
Epoch 33 out of 90
Training loss: tensor(0.7394)
Validation loss: tensor(0.7765)
Epoch 34 out of 90
Training loss: tensor(0.6595)
Validation loss: tensor(0.5824)
Epoch 35 out of 90
Training loss: tensor(0.5387)
Validation loss: tensor(0.5851)
Epoch 36 out of 90
Training loss: tensor(0.5255)
Validation loss: tensor(0.5863)
Epoch 37 out of 90
Training loss: tensor(0.5170)
Validation loss: tensor(0.5910)
Epoch 38 out of 90
Training loss: tensor(0.5210)
Validation loss: tensor(0.5949)
Epoch 39 out of 90
Training loss: tensor(0.5210)
Validation loss: tensor(0.6171)
Epoch 40 out of 90
Training loss: tensor(0.5145)
Validation loss: tensor(0.5910)
Epoch 41 out of 90
Training loss: tensor(0.5096)
Validation loss: tensor(0.5821)
Epoch 42 out of 90
Training loss: tensor(0.5197)
Validation loss: tensor(0.5838)
Epoch 43 out of 90
Training loss: tensor(0.5369)
Validation loss: tensor(0.6210)
Epoch 44 out of 90
Training loss: tensor(0.6148)
Validation loss: tensor(0.6397)
Epoch 45 out of 90
Training loss: tensor(0.5482)
Validation loss: tensor(0.6650)
Epoch 46 out of 90
Training loss: tensor(0.5349)
Validation loss: tensor(0.6348)
Epoch 47 out of 90
Training loss: tensor(0.5352)
Validation loss: tensor(0.6361)
Epoch 48 out of 90
Training loss: tensor(0.5321)
Validation loss: tensor(0.6400)
Epoch 49 out of 90
Training loss: tensor(0.5383)
Validation loss: tensor(0.6328)
Epoch 50 out of 90
Training loss: tensor(0.5386)
Validation loss: tensor(0.6155)
Epoch 51 out of 90
Training loss: tensor(0.5323)
Validation loss: tensor(0.6315)
Epoch 52 out of 90
Training loss: tensor(0.5245)
Validation loss: tensor(0.6261)
Epoch 53 out of 90
Training loss: tensor(0.5422)
Validation loss: tensor(0.6177)
Epoch 54 out of 90
Training loss: tensor(0.5311)
Validation loss: tensor(0.6105)
Epoch 55 out of 90
Training loss: tensor(0.5260)
Validation loss: tensor(0.6293)
Epoch 56 out of 90
Training loss: tensor(0.5284)
Validation loss: tensor(0.6116)
Epoch 57 out of 90
Training loss: tensor(0.5633)
Validation loss: tensor(0.7697)
Epoch 58 out of 90
Training loss: tensor(0.7322)
Validation loss: tensor(0.7519)
Epoch 59 out of 90
Training loss: tensor(0.6209)
Validation loss: tensor(0.7626)
Epoch 60 out of 90
Training loss: tensor(0.7076)
Validation loss: tensor(0.7577)
Epoch 61 out of 90
Training loss: tensor(0.7101)
Validation loss: tensor(0.7603)
Epoch 62 out of 90
Training loss: tensor(0.7095)
Validation loss: tensor(0.7563)
Epoch 63 out of 90
Training loss: tensor(0.7078)
Validation loss: tensor(0.7249)
Epoch 64 out of 90
Training loss: tensor(0.5829)
Validation loss: tensor(0.6439)
Epoch 65 out of 90
Training loss: tensor(0.5614)
Validation loss: tensor(0.6060)
Epoch 66 out of 90
Training loss: tensor(0.5593)
Validation loss: tensor(0.5821)
Epoch 67 out of 90
Training loss: tensor(0.5321)
Validation loss: tensor(0.6061)
Epoch 68 out of 90
Training loss: tensor(0.5373)
Validation loss: tensor(0.6289)
Epoch 69 out of 90
Training loss: tensor(0.5335)
Validation loss: tensor(0.6022)
Epoch 70 out of 90
Training loss: tensor(0.5462)
Validation loss: tensor(0.6868)
Epoch 71 out of 90
Training loss: tensor(0.6337)
Validation loss: tensor(0.7679)
Epoch 72 out of 90
Training loss: tensor(0.7406)
Validation loss: tensor(0.7967)
Epoch 73 out of 90
Training loss: tensor(0.7482)
Validation loss: tensor(0.7924)
Epoch 74 out of 90
Training loss: tensor(0.7492)
Validation loss: tensor(0.7847)
Epoch 75 out of 90
Training loss: tensor(0.7470)
Validation loss: tensor(0.7935)
Epoch 76 out of 90
Training loss: tensor(0.7481)
Validation loss: tensor(0.7997)
Epoch 77 out of 90
Training loss: tensor(0.7488)
Validation loss: tensor(0.7946)
Epoch 78 out of 90
Training loss: tensor(0.7492)
Validation loss: tensor(0.7951)
Epoch 79 out of 90
Training loss: tensor(0.7454)
Validation loss: tensor(0.8051)
Epoch 80 out of 90
Training loss: tensor(0.7445)
Validation loss: tensor(0.7841)
Epoch 81 out of 90
Training loss: tensor(0.7455)
Validation loss: tensor(0.7640)
Epoch 82 out of 90
Training loss: tensor(0.6785)
Validation loss: tensor(0.7512)
Epoch 83 out of 90
Training loss: tensor(0.7396)
Validation loss: tensor(0.7980)
Epoch 84 out of 90
Training loss: tensor(0.7282)
Validation loss: tensor(0.7782)
Epoch 85 out of 90
Training loss: tensor(0.6453)
Validation loss: tensor(0.7592)
Epoch 86 out of 90
Training loss: tensor(0.6535)
Validation loss: tensor(0.7774)
Epoch 87 out of 90
Training loss: tensor(0.6475)
Validation loss: tensor(0.7682)
Epoch 88 out of 90
Training loss: tensor(0.6460)
Validation loss: tensor(0.7575)
Epoch 89 out of 90
Training loss: tensor(0.6173)
Validation loss: tensor(0.7178)
Epoch 90 out of 90
Training loss: tensor(0.5952)
Validation loss: tensor(0.7207)
Training model for 90 took 13:24:18.486909 .
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward>)
tensor([[1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [0., 1.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.],
        [1., 0.]], device='cuda:0', grad_fn=<SoftmaxBackward>)
